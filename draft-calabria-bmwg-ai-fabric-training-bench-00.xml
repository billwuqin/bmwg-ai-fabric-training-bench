<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE rfc [
  <!ENTITY nbsp "&#160;">
  <!ENTITY zwsp "&#8203;">
  <!ENTITY nbhy "&#8209;">
  <!ENTITY wj   "&#8288;">
]>
<?xml-stylesheet type="text/xsl" href="rfc2629.xslt"?>

<rfc xmlns:xi="http://www.w3.org/2001/XInclude"
     category="info"
     docName="draft-calabria-bmwg-ai-fabric-training-bench-00"
     ipr="trust200902"
     obsoletes=""
     updates=""
     submissionType="IETF"
     xml:lang="en"
     tocInclude="true"
     tocDepth="4"
     symRefs="true"
     sortRefs="true"
     version="3">

  <!-- ====================================================== -->
  <!--  FRONT MATTER                                           -->
  <!-- ====================================================== -->
  <front>
    <title abbrev="AI Fabric Training Benchmarking">
      Benchmarking Methodology for AI Training Network Fabrics
    </title>

    <seriesInfo name="Internet-Draft"
                value="draft-calabria-bmwg-ai-fabric-training-bench-00"/>

    <author fullname="Fernando Calabria" initials="F." surname="Calabria">
      <organization>Cisco</organization>
      <address>
        <email>fcalabri@cisco.com</email>
      </address>
    </author>

    <author fullname="Carlos Pignataro" initials="C." surname="Pignataro">
      <organization>Blue Fern Consulting</organization>
      <address>
        <email>carlos@bluefern.consulting</email>
      </address>
    </author>

    <date month="February" year="2026"/>

    <area>Operations and Management</area>
    <workgroup>BMWG Working Group</workgroup>

    <keyword>benchmarking</keyword>
    <keyword>AI training</keyword>
    <keyword>network fabric</keyword>
    <keyword>RDMA</keyword>
    <keyword>RoCEv2</keyword>
    <keyword>UET</keyword>
    <keyword>collective communication</keyword>
    <keyword>AllReduce</keyword>
    <keyword>JCT</keyword>

    <abstract>
      <t>
        This document defines benchmarking terminology, methodologies, and Key
        Performance Indicators (KPIs) for evaluating Ethernet-based AI training
        network fabrics.
      </t>
      <t>
        As large-scale distributed AI/ML training clusters grow to tens of
        thousands of accelerators (GPUs/XPUs), the backend network fabric
        becomes the critical bottleneck determining job completion time (JCT),
        training throughput, and accelerator utilization.
      </t>
      <t>
        This document establishes vendor-independent, reproducible test
        procedures for benchmarking fabric-level performance under realistic AI
        training workloads, covering RDMA/RoCEv2 transport, the Ultra Ethernet
        Transport (UET) protocol defined by the UEC Specification 1.0,
        congestion management (PFC, ECN, DCQCN, CBFC), load balancing
        strategies (ECMP, DLB, packet spraying), collective communication
        patterns (AllReduce, AlltoAll, AllGather), and scale/soak testing.
      </t>
      <t>
        The methodology enables apples-to-apples comparison across different
        switch ASICs, vendor implementations, NIC transport stacks (RoCEv2 vs.
        UET), and fabric architectures (2-tier Clos, 3-tier Clos,
        rail-optimized).
      </t>
    </abstract>
  </front>

  <!-- ====================================================== -->
  <!--  MIDDLE MATTER                                          -->
  <!-- ====================================================== -->
  <middle>

    <!-- Section 1: Introduction -->
    <section anchor="intro" numbered="true" toc="default">
      <name>Introduction</name>
      <t>
        The rapid growth of distributed AI/ML training workloads has
        fundamentally changed the performance requirements for data center
        network fabrics. Unlike traditional data center traffic characterized by
        diverse flow sizes and protocols, AI training workloads generate highly
        synchronized, bandwidth-intensive, east-west traffic patterns dominated
        by collective communication operations (AllReduce, AlltoAll, AllGather).
        These workloads impose unique demands: lossless transport (via RoCEv2
        over RDMA), ultra-low tail latency, near-perfect load balancing across
        all fabric paths, and the ability to absorb coordinated micro-bursts
        from thousands of accelerators simultaneously.
      </t>
      <t>
        Existing BMWG methodologies, while foundational, do not adequately
        address the characteristics of AI training fabrics. <xref
        target="RFC2544"/> defines benchmarking for general network interconnect
        devices but does not account for RDMA transport semantics, collective
        communication patterns, or the unique congestion dynamics of
        GPU-to-GPU traffic. <xref target="RFC8238"/> and <xref target="RFC8239"/>
        establish data center benchmarking terminology and methodology but
        predate the AI fabric paradigm and do not address RoCEv2-specific
        behaviors such as Priority Flow Control (PFC) interactions, DCQCN
        congestion control convergence, or the impact of load balancing
        strategies on Job Completion Time (JCT).
      </t>
      <t>
        The EVPN benchmarking methodology <xref target="EVPN-BENCH"/> provides
        a structural template for service-oriented benchmarking but is scoped
        to L2VPN services rather than RDMA fabrics.
      </t>
      <t>
        This document fills the gap by defining a comprehensive benchmarking
        methodology specifically designed for AI training network fabrics. The
        methodology is vendor-independent and ASIC-agnostic, enabling fair
        comparison across implementations from current-generation switch silicon
        including deep-buffer VOQ ASICs and others. It covers the full stack
        from raw RDMA transport performance (both legacy RoCEv2 and the emerging
        UEC Ultra Ethernet Transport protocol) through collective communication
        efficiency to end-to-end Job Completion Time, providing operators and
        evaluators with the metrics needed to select, validate, and optimize AI
        fabric deployments.
      </t>

      <!-- Section 1.1 -->
      <section anchor="req-lang" numbered="true" toc="default">
        <name>Requirements Language</name>
        <t>
          The key words <bcp14>MUST</bcp14>, <bcp14>MUST NOT</bcp14>,
          <bcp14>REQUIRED</bcp14>, <bcp14>SHALL</bcp14>, <bcp14>SHALL
          NOT</bcp14>, <bcp14>SHOULD</bcp14>, <bcp14>SHOULD NOT</bcp14>,
          <bcp14>RECOMMENDED</bcp14>, <bcp14>MAY</bcp14>, and
          <bcp14>OPTIONAL</bcp14> in this document are to be interpreted as
          described in BCP&nbsp;14 <xref target="RFC2119"/> <xref
          target="RFC8174"/> when, and only when, they appear in all capitals,
          as shown here.
        </t>
      </section>

      <!-- Section 1.2 -->
      <section anchor="scope" numbered="true" toc="default">
        <name>Scope and Applicability</name>
        <t>
          This document applies to Ethernet-based AI training backend network
          fabrics employing RoCEv2 and/or UEC Ultra Ethernet Transport (UET)
          protocols. The scope includes leaf-spine (2-tier Clos) and
          leaf-spine-superspine (3-tier Clos) topologies.
        </t>
        <t>
          InfiniBand fabrics are explicitly out of scope, though many KPIs
          defined herein may be adapted for IB benchmarking by future documents.
          The DUT is the network fabric itself (the collection of switches and
          interconnecting links), not individual accelerators or host NICs,
          though host-side configuration (including NIC transport stack: RoCEv2
          vs. UET, and UEC compliance profile) <bcp14>MUST</bcp14> be
          documented as it materially affects results.
        </t>
        <t>
          The methodology is designed for controlled laboratory environments per
          the BMWG charter; it is <bcp14>NOT</bcp14> intended for production
          network measurement.
        </t>
      </section>

      <!-- Section 1.3 -->
      <section anchor="related-work" numbered="true" toc="default">
        <name>Relationship to Existing BMWG Work</name>
        <t>
          This document extends and complements the following BMWG and other
          specifications:
        </t>
        <table anchor="tbl-related-work" align="left">
          <name>Relationship to Existing BMWG Documents</name>
          <thead>
            <tr><th>Document</th><th>Relationship</th></tr>
          </thead>
          <tbody>
            <tr>
              <td>RFC 1242</td>
              <td>Base terminology for network benchmarking; terms reused herein</td>
            </tr>
            <tr>
              <td>RFC 2544</td>
              <td>Base methodology; throughput/latency/loss tests adapted for RDMA</td>
            </tr>
            <tr>
              <td>RFC 2889</td>
              <td>LAN switching methodology; MAC learning concepts adapted for ARP/ND scale</td>
            </tr>
            <tr>
              <td>RFC 8238</td>
              <td>Data center terminology; buffer, congestion, and microburst terms extended</td>
            </tr>
            <tr>
              <td>RFC 8239</td>
              <td>Data center methodology; line-rate and buffer tests adapted for RoCEv2</td>
            </tr>
            <tr>
              <td>RFC 9004</td>
              <td>Back-to-back frame updates; burst absorption methodology referenced</td>
            </tr>
            <tr>
              <td>draft-gaikwad-llm-* <xref target="LLM-BENCH"/></td>
              <td>Complementary document benchmarking the inference serving stack; treats the
              network fabric as an opaque SUT component. This document benchmarks the fabric
              itself. The two documents MAY be used together but MUST NOT be combined in a
              single benchmarking report without explicit section demarcation.</td>
            </tr>
            <tr>
              <td>UEC Spec 1.0 <xref target="UEC-1.0"/></td>
              <td>UET protocol specification; transport services, congestion control, and
              link-layer enhancements benchmarked in <xref target="uet-bench"/></td>
            </tr>
          </tbody>
        </table>
      </section>
    </section>

    <!-- Section 2: Terminology -->
    <section anchor="terminology" numbered="true" toc="default">
      <name>Terminology and Definitions</name>
      <t>
        The following terms are defined for use in this document. Where a term
        overlaps with <xref target="RFC1242"/> or <xref target="RFC8238"/>, the
        definition herein takes precedence in the context of AI fabric
        benchmarking.
      </t>
      <dl newline="true" spacing="normal">
        <dt>AI Fabric</dt>
        <dd>The dedicated Ethernet backend network interconnecting accelerators
        (GPUs/XPUs) for distributed AI training, typically a non-blocking Clos
        topology running RoCEv2.</dd>

        <dt>JCT (Job Completion Time)</dt>
        <dd>The wall-clock duration from the start of a training job (or
        benchmark iteration) until all participating accelerators complete their
        work, inclusive of all computation and communication phases.</dd>

        <dt>DUT Fabric</dt>
        <dd>The complete system under test: all leaf switches, spine switches,
        superspine switches (if applicable), and interconnecting links forming
        the AI training fabric.</dd>

        <dt>Roofline JCT</dt>
        <dd>The theoretical minimum JCT assuming perfect (zero-contention)
        network behavior, computed from computation time plus serialization
        delay at link rate with zero queuing.</dd>

        <dt>JCT Ratio</dt>
        <dd>Measured JCT divided by Roofline JCT; a value of 1.0 indicates no
        network-induced overhead; values &gt;1.0 indicate fabric
        inefficiency.</dd>

        <dt>BusBW (Bus Bandwidth)</dt>
        <dd>The effective data throughput achieved per accelerator during a
        collective operation, calculated as (data_size * algo_factor) / time,
        where algo_factor accounts for the collective algorithm (e.g.,
        2*(n-1)/n for ring AllReduce).</dd>

        <dt>QP (Queue Pair)</dt>
        <dd>The fundamental RDMA communication endpoint comprising a Send Queue
        and Receive Queue; multiple QPs per source-destination pair increase
        ECMP entropy.</dd>

        <dt>Incast Ratio</dt>
        <dd>The ratio of senders to receivers in a communication pattern, e.g.,
        N:1 incast where N sources simultaneously target one destination
        port.</dd>

        <dt>MMR (Max-Mean Ratio)</dt>
        <dd>Ratio of the flow count on the most loaded link to the average flow
        count per link; quantifies ECMP imbalance (1.0 is perfect).</dd>

        <dt>PFC Pause Event</dt>
        <dd>A single Priority Flow Control PAUSE frame transmitted on a
        priority; indicates the upstream device's buffers are approaching
        capacity for that priority class.</dd>

        <dt>ECN Marking Ratio</dt>
        <dd>Percentage of packets marked with Congestion Experienced (CE) in
        the IP ECN field over a measurement interval.</dd>

        <dt>Collective Operation</dt>
        <dd>A coordinated communication pattern executed across all accelerators
        in a training group: AllReduce (gradient synchronization), AlltoAll
        (expert routing in MoE), AllGather (parameter distribution).</dd>

        <dt>DCQCN</dt>
        <dd>Data Center Quantized Congestion Notification: the combination of
        ECN and PFC used with RoCEv2 for end-to-end congestion control.</dd>

        <dt>Packet Spray</dt>
        <dd>A load balancing strategy that distributes individual packets of a
        single flow across all available ECMP paths; maximizes link utilization
        but may cause packet reordering.</dd>

        <dt>DLB/Flowlet</dt>
        <dd>Dynamic Load Balancing using flowlet detection: reroutes traffic
        when a flow has been idle longer than a flowlet gap threshold, reducing
        reordering risk vs. packet spray.</dd>

        <dt>Zero-Impact Failover</dt>
        <dd>Sub-microsecond automatic path convergence upon link or switch
        failure, resulting in no measurable impact to JCT.</dd>

        <dt>UET (Ultra Ethernet Transport)</dt>
        <dd>The connectionless RDMA transport protocol defined by the Ultra
        Ethernet Consortium (UEC) Specification 1.0, designed as a
        next-generation replacement for RoCEv2 in AI/HPC fabrics.</dd>

        <dt>PDC (Packet Delivery Context)</dt>
        <dd>The ephemeral, lightweight transport endpoint in UET, analogous to
        but distinct from an RDMA Queue Pair (QP); PDCs are connectionless and
        require no explicit setup handshake before data transmission.</dd>

        <dt>ROD (Reliable Ordered Delivery)</dt>
        <dd>A UET transport service providing reliable, in-order packet
        delivery, semantically equivalent to RoCEv2 RC mode; suitable for
        legacy RDMA applications requiring strict ordering.</dd>

        <dt>RUD (Reliable Unordered Delivery)</dt>
        <dd>A UET transport service optimized for AI workloads that provides
        reliable delivery without maintaining packet ordering; enables native
        packet spraying without reorder buffer overhead at the receiver NIC.</dd>

        <dt>RUDI (Reliable Unordered Delivery for Idempotent operations)</dt>
        <dd>A UET transport service optimized for operations that are safe to
        execute more than once (e.g., RDMA Writes), allowing simplified
        retransmission logic.</dd>

        <dt>UUD (Unreliable Unordered Delivery)</dt>
        <dd>A UET transport service for best-effort, unordered traffic; lowest
        overhead, suitable for telemetry or speculative operations.</dd>

        <dt>LLR (Link Layer Retry)</dt>
        <dd>An optional UEC link-layer enhancement providing fast per-hop error
        recovery (sub-microsecond) at the Ethernet link layer, reducing the
        load on transport-layer retransmission.</dd>

        <dt>Packet Trimming</dt>
        <dd>An optional UEC link-layer enhancement where congested switches
        transmit only the packet header (trimmed packet) instead of dropping the
        full packet, enabling the receiver to detect loss faster and trigger
        selective retransmission with reduced bandwidth waste.</dd>

        <dt>CBFC (Credit-Based Flow Control)</dt>
        <dd>An optional UEC link-layer mechanism for buffer management using
        credit-based admission, an alternative to PFC that avoids head-of-line
        blocking and PFC storm propagation.</dd>

        <dt>UEC Profile</dt>
        <dd>A defined subset of UET features targeting a specific use case: AI
        Base (core AI training), AI Full (AI Base + deferred send, exact-match
        tagging, extended atomics), or HPC (latency-optimized for traditional
        HPC).</dd>

        <dt>Entropy Value</dt>
        <dd>A per-packet field in UET used to distribute packets across ECMP
        paths; provides explicit spray entropy independent of the 5-tuple,
        improving multipath utilization for AI traffic.</dd>
      </dl>
    </section>

    <!-- Section 3: Topology -->
    <section anchor="topology" numbered="true" toc="default">
      <name>Test Topology and Architecture</name>

      <section anchor="ref-topologies" numbered="true" toc="default">
        <name>Reference Fabric Topologies</name>
        <t>
          Three reference topologies are defined. All tests <bcp14>MUST</bcp14>
          specify which topology is used. Results obtained under different
          topologies are <bcp14>NOT</bcp14> directly comparable without
          normalization.
        </t>

        <section anchor="topo-a" numbered="true" toc="default">
          <name>Topology A: 2-Tier Clos (Leaf-Spine)</name>
          <artwork align="center" name="" type="ascii-art"><![CDATA[
+--------+ +--------+ +--------+ +--------+
| Spine1 | | Spine2 | | Spine3 | | SpineN |
+--++--+  +--++--+  +--++--+  +--++--+
   ||         ||         ||         ||
   ||  Full Mesh Interconnect        ||
   ||  (ECMP / DLB / Spray)         ||
   ||         ||         ||         ||
+--++--+  +--++--+  +--++--+  +--++--+
| Leaf 1 | | Leaf 2 | | Leaf 3 | | Leaf N |
+--++--+  +--++--+  +--++--+  +--++--+
   ||         ||         ||         ||
[GPU/XPU] [GPU/XPU] [GPU/XPU] [GPU/XPU]
Hosts w/  Hosts w/  Hosts w/  Hosts w/
RoCEv2 NIC RoCEv2 NIC RoCEv2 NIC RoCEv2 NIC
          ]]></artwork>
          <t>
            The DUT boundary encompasses all leaf and spine switches and their
            interconnecting links. Traffic generators or actual GPU hosts connect
            at the leaf layer.
          </t>
        </section>

        <section anchor="topo-b" numbered="true" toc="default">
          <name>Topology B: 3-Tier Clos (Leaf-Spine-Superspine)</name>
          <t>
            For clusters exceeding thousands of accelerators, a superspine layer
            is added. Each pod consists of a leaf-spine fabric, and pods are
            interconnected via superspine switches. This topology scales to
            32,000+ accelerators at 800GbE with current-generation ASICs. The
            DUT boundary encompasses all three tiers.
          </t>
        </section>

        <section anchor="topo-c" numbered="true" toc="default">
          <name>Topology C: Rail-Optimized</name>
          <artwork align="center" name="" type="ascii-art"><![CDATA[
                         SPINE LAYER
+--------+ +--------+ +--------+ +--------+
| Spine1 | | Spine2 | | Spine3 | | SpineN |
+--+--+--+ +--+--+--+ +--+--+--+ +--+--+--+
   |     Full Mesh Interconnect (ECMP/Spray)   |
+--+--+--+ +--+--+--+ +--+--+--+ +--+--+--+
| Rail-0 | | Rail-1 | | Rail-2 | | Rail-7 |  RAIL (LEAF) LAYER
|  Leaf  | |  Leaf  | |  Leaf  | |  Leaf  |  one switch per NIC
+--+--+--+ +--+--+--+ +--+--+--+ +--+--+--+
   |  |       |  |       |  |       |  |
 NIC-0 NIC-0 NIC-1 NIC-1 NIC-2 NIC-2 NIC-7 NIC-7
   |  |       |  |       |  |       |  |
+--+--+--+ +--+--+--+ +--+--+--+ +--+--+--+
| Host A | | Host B | | Host C | | Host D |  GPU HOSTS
| GPU[0] | | GPU[0] | | GPU[0] | | GPU[0] |  (each host has
| GPU[1] | | GPU[1] | | GPU[1] | | GPU[1] |   8 NICs, one
|  ...   | |  ...   | |  ...   | |  ...   |   per rail)
| GPU[7] | | GPU[7] | | GPU[7] | | GPU[7] |
+--------+ +--------+ +--------+ +--------+
|<------- Rail-0 ------->|        |<-Rail-7->|
(all NIC-0s share Rail-0 leaf)  (all NIC-7s share Rail-7 leaf)
          ]]></artwork>
          <t>
            In rail-optimized topologies, each NIC on a multi-NIC host connects
            to a dedicated leaf switch ("rail"), and rails are interconnected at
            the spine layer. This topology co-optimizes network locality with the
            collective communication library (NCCL/RCCL). The DUT boundary and
            rail mapping <bcp14>MUST</bcp14> be fully documented.
          </t>
        </section>
      </section>

      <section anchor="dut-id" numbered="true" toc="default">
        <name>Device Under Test (DUT) Identification</name>
        <t>
          The following parameters <bcp14>MUST</bcp14> be recorded and reported
          for all DUT components:
        </t>
        <table anchor="tbl-dut" align="left">
          <name>Required DUT Parameters</name>
          <thead>
            <tr><th>Parameter</th><th>Description</th><th>Example</th></tr>
          </thead>
          <tbody>
            <tr><td>Switch Vendor/Model</td><td>Vendor name, product family, model number</td><td>&lt;Vendor&gt; &lt;Family&gt; &lt;Model&gt;</td></tr>
            <tr><td>Switch ASIC</td><td>Silicon vendor, ASIC family, revision</td><td>&lt;Silicon Vendor&gt; &lt;ASIC Family&gt; &lt;Rev&gt;</td></tr>
            <tr><td>NOS Version</td><td>Network operating system name and version</td><td>&lt;NOS Name&gt; &lt;Version&gt;</td></tr>
            <tr><td>Port Speed</td><td>Per-port line rate</td><td>400GbE, 800GbE</td></tr>
            <tr><td>Buffer Architecture</td><td>Shared/dedicated, total buffer per ASIC/port</td><td>32MB shared + 16MB VOQ per port</td></tr>
            <tr><td>Optics/Cables</td><td>Transceiver type, cable type and length</td><td>OSFP 400G-DR4, DAC 3m copper</td></tr>
            <tr><td>NIC Vendor/Model</td><td>RDMA NIC vendor, model, firmware</td><td>&lt;NIC Vendor&gt; &lt;Model&gt; &lt;Form Factor&gt; &lt;Speed&gt;</td></tr>
            <tr><td>NIC Firmware</td><td>NIC firmware version</td><td>&lt;Firmware Version&gt;</td></tr>
            <tr><td>Host Config</td><td>OS, CCL lib version, driver version, BIOS settings</td><td>&lt;OS Name&gt; &lt;Version&gt;, &lt;CCL Name&gt; &lt;Version&gt;, &lt;OFED Version&gt;</td></tr>
          </tbody>
        </table>
      </section>

      <section anchor="tgen-req" numbered="true" toc="default">
        <name>Traffic Generator Requirements</name>

        <section anchor="tgen-func" numbered="true" toc="default">
          <name>Mandatory Functional Capabilities</name>
          <t>
            The traffic generator <bcp14>MUST</bcp14> be capable of emulating
            RoCEv2 transport including Queue Pair (QP) establishment, RDMA
            Write/Read operations, ECN processing, and DCQCN rate control. The
            generator <bcp14>MUST</bcp14> support configurable QP scaling (1 to
            256 QPs per source-destination pair), programmable collective
            communication patterns (AllReduce, AlltoAll, AllGather with
            configurable message sizes), and nanosecond-precision timestamping.
          </t>
        </section>

        <section anchor="tgen-accuracy" numbered="true" toc="default">
          <name>Minimum Measurement Accuracy Requirements</name>
          <table anchor="tbl-tgen-accuracy" align="left">
            <name>Traffic Generator Minimum Accuracy Requirements</name>
            <thead>
              <tr><th>Parameter</th><th>Minimum Requirement</th></tr>
            </thead>
            <tbody>
              <tr><td>Timestamp accuracy</td><td>&lt;= 100 nanoseconds</td></tr>
              <tr><td>Frame rate accuracy</td><td>+/- 0.1% of specified rate</td></tr>
              <tr><td>QP scaling range</td><td>1 to 256 QPs per src-dst pair</td></tr>
              <tr><td>Message size range</td><td>1 KB to 8 GB</td></tr>
              <tr><td>Flow counter resolution</td><td>Per-flow byte and packet counts</td></tr>
              <tr><td>Loss measurement</td><td>0 ppm resolution</td></tr>
              <tr><td>Burst generation</td><td>1-1000 frames at line rate</td></tr>
            </tbody>
          </table>
        </section>

        <section anchor="tgen-impl" numbered="true" toc="default">
          <name>Acceptable Implementations</name>
          <t>
            Qualifying platforms include: (a) dedicated hardware traffic
            generators capable of line-rate RDMA emulation with the accuracy
            requirements of <xref target="tgen-accuracy"/>, or (b) instrumented
            GPU clusters with RDMA tooling, provided the host configuration
            (CPU, GPU, PCIe topology, BIOS power management settings) is fully
            documented. When GPU clusters are used as traffic sources, any
            non-fabric overhead in timing measurements <bcp14>MUST</bcp14> be
            quantified and reported separately.
          </t>
        </section>
      </section>
    </section>

    <!-- Section 4: KPI Framework -->
    <section anchor="kpi-framework" numbered="true" toc="default">
      <name>KPI Framework and Metrics Taxonomy</name>
      <t>
        NOTE: Target values in this section are NON-NORMATIVE illustrative
        reference points derived from current industry practice. They do NOT
        constitute benchmarking acceptance criteria or performance requirements.
        Per BMWG charter, defining acceptance criteria is explicitly out of
        scope. Implementers <bcp14>MAY</bcp14> use these values as contextual
        references; they <bcp14>MUST NOT</bcp14> be used as pass/fail
        thresholds.
      </t>

      <section anchor="primary-kpis" numbered="true" toc="default">
        <name>Primary KPIs</name>
        <table anchor="tbl-primary-kpis" align="left">
          <name>Primary KPIs</name>
          <thead>
            <tr><th>KPI</th><th>Unit</th><th>Definition</th><th>Report</th></tr>
          </thead>
          <tbody>
            <tr>
              <td>Job Completion Time (JCT)</td>
              <td>seconds</td>
              <td>Wall-clock time for benchmark iteration including compute + communication phases</td>
              <td>Minimize; report as absolute and as JCT Ratio vs. Roofline</td>
            </tr>
            <tr>
              <td>JCT Ratio</td>
              <td>dimensionless</td>
              <td>Measured JCT / Roofline JCT; fabric overhead factor</td>
              <td>&lt;= 1.05 (better than &lt;= 1.15 OK)</td>
            </tr>
            <tr>
              <td>Bus Bandwidth (BusBW)</td>
              <td>Gbps per accelerator</td>
              <td>Effective per-accelerator throughput during collective operation</td>
              <td>&gt;= 90% of NIC line rate for intra-pod traffic</td>
            </tr>
            <tr>
              <td>Aggregate Throughput</td>
              <td>Tbps</td>
              <td>Total fabric goodput during collective communication phase</td>
              <td>&gt;= 95% of bisection BW</td>
            </tr>
            <tr>
              <td>Packet Drop Rate</td>
              <td>ppm (parts per million)</td>
              <td>Frames lost end-to-end not retransmitted</td>
              <td>0 ppm (lossless); report any non-zero value</td>
            </tr>
            <tr>
              <td>Tail Latency (P99 / P99.9)</td>
              <td>microseconds</td>
              <td>99th / 99.9th percentile one-way fabric latency</td>
              <td>Report absolute; minimize</td>
            </tr>
          </tbody>
        </table>
      </section>

      <section anchor="secondary-kpis" numbered="true" toc="default">
        <name>Secondary KPIs</name>
        <table anchor="tbl-secondary-kpis" align="left">
          <name>Secondary KPIs</name>
          <thead>
            <tr><th>KPI</th><th>Unit</th><th>Definition</th></tr>
          </thead>
          <tbody>
            <tr><td>ECN Marking Ratio</td><td>%</td><td>Percentage of packets marked CE over measurement interval</td></tr>
            <tr><td>PFC Pause Count</td><td>events/sec</td><td>Rate of PFC PAUSE frames generated per priority per port</td></tr>
            <tr><td>PFC Pause Duration</td><td>microseconds</td><td>Cumulative time a port spends in PFC-paused state per interval</td></tr>
            <tr><td>RDMA Retransmission Rate</td><td>retx/sec</td><td>NIC-level retransmissions due to timeouts or NAKs</td></tr>
            <tr><td>ECMP Imbalance (MMR)</td><td>dimensionless</td><td>Max-Mean Ratio of flow counts across parallel uplinks</td></tr>
            <tr><td>Jain Fairness Index (JFI)</td><td>0.0 - 1.0</td><td>Fairness of traffic distribution across fabric links; 1.0 = perfect</td></tr>
            <tr><td>Queue Depth (P95/Max)</td><td>bytes or cells</td><td>95th percentile and maximum egress queue occupancy per port</td></tr>
            <tr><td>Congestion Control Convergence</td><td>microseconds</td><td>Time from congestion onset to DCQCN rate stabilization</td></tr>
            <tr><td>Out-of-Order Packet Rate</td><td>pkt/sec</td><td>Packets delivered out of sequence; relevant for packet spray configurations</td></tr>
            <tr><td>CTS/ACK Delay</td><td>microseconds</td><td>Delay for control messages (Clear-to-Send, ACKs) in receiver-driven schemes</td></tr>
          </tbody>
        </table>
      </section>

      <section anchor="fabric-health" numbered="true" toc="default">
        <name>Fabric Health Indicators</name>
        <table anchor="tbl-health" align="left">
          <name>Fabric Health Indicators</name>
          <thead>
            <tr><th>Indicator</th><th>Unit</th><th>Definition</th></tr>
          </thead>
          <tbody>
            <tr><td>Switch CPU Utilization</td><td>%</td><td>Average and peak CPU usage on DUT control plane during test</td></tr>
            <tr><td>Switch Memory Utilization</td><td>%</td><td>Average and peak memory usage, including FIB/MAC table occupancy</td></tr>
            <tr><td>FIB/Route Convergence Time</td><td>ms</td><td>Time to converge routing after topology change</td></tr>
            <tr><td>Link Flap Count</td><td>events</td><td>Spurious link state changes during test period</td></tr>
            <tr><td>CRC/FCS Error Rate</td><td>errors/sec</td><td>Physical layer errors indicating cable or optics issues</td></tr>
            <tr><td>Power Consumption</td><td>Watts</td><td>Per-switch and per-port power draw under test load</td></tr>
          </tbody>
        </table>
      </section>
    </section>

    <!-- Section 5: RDMA Transport Benchmarks -->
    <section anchor="rdma-bench" numbered="true" toc="default">
      <name>Test Category 1: RDMA Transport Benchmarks</name>
      <t>
        These tests establish baseline fabric performance for RDMA traffic
        independent of collective communication patterns. They extend <xref
        target="RFC2544"/> and <xref target="RFC8239"/> methodology for RoCEv2
        semantics.
      </t>

      <section anchor="rdma-throughput" numbered="true" toc="default">
        <name>Baseline Throughput</name>
        <t>
          Objective: Determine the maximum sustainable RDMA Write throughput
          through the DUT fabric at each tested message size, analogous to
          <xref target="RFC2544"/> throughput testing but using RoCEv2
          transport.
        </t>
        <t>
          Procedure: Configure N host pairs, each establishing Q Queue Pairs per
          pair. Initiate RDMA Write operations with the specified message size
          and measure aggregate goodput. The test <bcp14>MUST</bcp14> be run for
          at least 60 seconds at each rate. A binary search (per <xref
          target="RFC2544"/> Section 26.1) <bcp14>SHOULD</bcp14> be used to
          find the maximum rate with zero packet loss. The throughput
          <bcp14>MUST</bcp14> be measured for the following message sizes: 64B,
          256B, 1KB, 4KB, 64KB, 256KB, 1MB, 4MB. The test <bcp14>MUST</bcp14>
          be repeated for QP counts of 1, 4, 16, and 32 per source-destination
          pair. Test with both unidirectional and bidirectional traffic.
        </t>
        <t>
          Reporting: Report aggregate throughput (Tbps), per-port utilization
          (%), and throughput efficiency (measured/theoretical). Present results
          as a table indexed by message size and QP count, and as a graph with
          message size on the X-axis and throughput on the Y-axis.
        </t>
      </section>

      <section anchor="rdma-latency" numbered="true" toc="default">
        <name>Latency Characterization</name>
        <t>
          Objective: Determine the one-way and round-trip RDMA latency
          distribution through the DUT fabric at the throughput rate determined
          in <xref target="rdma-throughput"/>.
        </t>
        <t>
          Procedure: At the determined throughput rate for each message size,
          inject tagged frames at 60 seconds into a 120-second stream (per <xref
          target="RFC2544"/> Section 26.2). Record timestamps at nanosecond
          precision. The test <bcp14>MUST</bcp14> report minimum, mean, P50,
          P95, P99, P99.9, and maximum latency values. Repeat at least 20 times
          and report the average of recorded values. Conduct tests under both
          zero-load (single QP, single flow) and loaded (full fabric
          utilization) conditions to characterize the latency increase under
          contention.
        </t>
        <t>
          Reporting: Tabulate latency statistics for each message size.
          Additionally, provide a latency histogram and a CDF plot. Report the
          latency increase factor (loaded/unloaded) as a key diagnostic metric.
        </t>
      </section>

      <section anchor="rdma-burst" numbered="true" toc="default">
        <name>Back-to-Back Burst Absorption</name>
        <t>
          Objective: Characterize the DUT fabric's ability to absorb
          back-to-back RDMA bursts without loss, extending <xref
          target="RFC9004"/> methodology for RoCEv2.
        </t>
        <t>
          Procedure: Transmit bursts of RDMA Write frames at line rate with
          minimum inter-frame gap. Increase burst length until the first frame
          loss is detected. The back-to-back value is the maximum burst length
          (in bytes and frames) handled without loss. Test at incast ratios of
          2:1, 4:1, 8:1, 16:1, and 32:1. Repeat at least 50 times per burst
          length and report the average.
        </t>
        <t>
          This test directly measures the effective buffer depth available for
          absorbing AI training microbursts.
        </t>
        <t>
          Reporting: Report burst absorption capacity in frames and bytes for
          each message size and incast ratio. Plot burst capacity vs. incast
          ratio to characterize the buffer utilization efficiency.
        </t>
      </section>
    </section>

    <!-- Section 6: UET Benchmarks -->
    <section anchor="uet-bench" numbered="true" toc="default">
      <name>Test Category 1A: UEC Transport Protocol Benchmarks</name>
      <t>
        The Ultra Ethernet Consortium (UEC) Specification 1.0 defines the Ultra
        Ethernet Transport (UET) protocol, a connectionless RDMA transport
        designed to replace RoCEv2 for AI/HPC workloads. UET introduces four
        transport services (ROD, RUD, RUDI, UUD), native per-packet multipath
        spraying without reorder buffer requirements (via RUD),
        sender-plus-receiver congestion control, and optional link-layer
        enhancements (LLR, Packet Trimming, CBFC). These tests benchmark
        UET-specific behaviors and provide direct comparative measurements
        against RoCEv2 on identical topologies. All UET tests use the libfabric
        API over UET and run on UEC 1.0-compliant NICs.
      </t>
      <t>
        The UEC compliance profile (AI Base, AI Full, or HPC) used during
        testing <bcp14>MUST</bcp14> be documented.
      </t>

      <section anchor="uet-throughput" numbered="true" toc="default">
        <name>UET Throughput by Transport Service</name>
        <t>
          Objective: Determine the maximum sustainable throughput achievable
          under each UET transport service (ROD, RUD, RUDI, UUD) and compare to
          RoCEv2 RC/UC modes on the same DUT fabric.
        </t>
        <t>
          Procedure: Configure N host pairs using UEC 1.0-compliant NICs. For
          each transport service, establish Packet Delivery Contexts (PDCs) and
          initiate bulk data transfers using the libfabric fi_write verb. Measure
          aggregate goodput at each message size: 64B, 256B, 1KB, 4KB, 64KB,
          256KB, 1MB, 4MB. Apply the binary search procedure from <xref
          target="RFC2544"/> Section 26.1 to find the maximum zero-loss
          throughput. The test <bcp14>MUST</bcp14> be run for at least 60
          seconds per trial. PDC counts per source-destination pair
          <bcp14>MUST</bcp14> be varied: 1, 4, 16, 32. A parallel RoCEv2 test
          series using the same host pairs, message sizes, and QP counts
          <bcp14>MUST</bcp14> be executed for direct comparison. Both
          unidirectional and bidirectional configurations <bcp14>MUST</bcp14> be
          tested.
        </t>
        <t>
          Reporting: Present a comparison table indexed by (transport_service,
          message_size, PDC_count) with columns for UET throughput (Gbps),
          RoCEv2 throughput (Gbps), and delta (%).
        </t>
      </section>

      <section anchor="uet-latency" numbered="true" toc="default">
        <name>UET Latency Characterization</name>
        <t>
          Objective: Measure the one-way and round-trip latency distribution for
          UET transport services and quantify any latency differential versus
          RoCEv2, with particular attention to connectionless PDC establishment
          overhead.
        </t>
        <t>
          Procedure: Measure latency for: (a) steady-state PDC transfers (PDC
          already established), (b) first-packet latency (PDC establishment +
          first data packet, measuring UET's "data before handshake"
          optimization), (c) zero-load baseline (single PDC, single host pair).
          Test under ROD (ordered) and RUD (unordered) services separately to
          isolate reordering-related latency components.
        </t>
        <t>
          Reporting: Tabulate latency statistics for each (transport_service,
          message_size, load_condition) tuple. Plot latency CDF for UET ROD,
          UET RUD, and RoCEv2 RC side-by-side. Report the first-packet latency
          advantage of UET's connectionless model over RoCEv2 QP establishment.
        </t>
      </section>

      <section anchor="uet-spray" numbered="true" toc="default">
        <name>Packet Spray Efficacy Under UET RUD</name>
        <t>
          Objective: Quantify the load balancing improvement achieved by UET's
          native per-packet spray with RUD, which eliminates the receiver
          reorder buffer requirement that constrains packet spray effectiveness
          under RoCEv2.
        </t>
        <t>
          Procedure: Configure the DUT fabric with per-packet load balancing.
          Generate bulk transfer traffic using: (a) UET RUD with per-packet
          spray, (b) UET ROD with per-packet spray, (c) RoCEv2 RC with
          per-packet spray, (d) RoCEv2 RC with standard ECMP. For each
          configuration, measure: Max-Mean Ratio (MMR), Jain Fairness Index
          (JFI), out-of-order packet delivery rate, RDMA/UET retransmission
          rate, and effective goodput after retransmissions.
        </t>
        <t>
          Reporting: The key result is the out-of-order rate: UET RUD
          <bcp14>SHOULD</bcp14> achieve zero host-visible reordering despite
          per-packet spray because the transport layer natively tolerates
          unordered delivery. Report the entropy value distribution to verify
          uniform ECMP path utilization.
        </t>
      </section>

      <section anchor="uet-cc" numbered="true" toc="default">
        <name>UET Congestion Control Benchmarks</name>
        <t>
          Objective: Evaluate UET's dual-sided (sender + receiver) congestion
          control mechanism under N:1 incast conditions and compare convergence
          behavior to RoCEv2 DCQCN.
        </t>
        <t>
          Procedure: Measure: (a) incast throughput with N = {2, 4, 8, 16, 32,
          64}, (b) convergence time after doubling active senders, (c) PFC
          avoidance with PFC disabled, (d) receiver credit utilization.
        </t>
        <t>
          Reporting: Tabulate incast throughput, convergence time, peak queue
          depth, PFC event count, and packet drop rate for UET vs. DCQCN at
          each incast ratio. The critical differentiator is the PFC-free
          operation test: report whether UET achieves zero application-visible
          loss without PFC and the retransmission overhead cost.
        </t>
      </section>

      <section anchor="uet-lle" numbered="true" toc="default">
        <name>Link Layer Enhancement Benchmarks</name>
        <t>
          Objective: Measure the performance impact of UEC optional link-layer
          enhancements: Link Layer Retry (LLR), Packet Trimming (PT), and
          Credit-Based Flow Control (CBFC).
        </t>
        <t>
          Procedure: (a) LLR Retry Latency: inject controlled bit errors;
          measure LLR retry success rate and latency (expected
          sub-microsecond per hop). (b) Packet Trimming Effectiveness: configure
          a 2:1 oversubscription bottleneck; measure time from congestion onset
          to first retransmission request. (c) CBFC vs. PFC: configure identical
          N:1 (N=32) incast scenarios; measure head-of-line blocking duration,
          pause propagation hops, and throughput of non-congested flows.
        </t>
        <t>
          Reporting: For each enhancement, present a before/after comparison
          table. Note which features are hardware-supported vs.
          software-emulated on the tested NIC/switch.
        </t>
      </section>

      <section anchor="uet-collective" numbered="true" toc="default">
        <name>UET Collective Communication Performance</name>
        <t>
          Objective: Measure collective communication (AllReduce, AlltoAll,
          AllGather) performance over UET and compare to RoCEv2, isolating the
          transport protocol's contribution to collective efficiency.
        </t>
        <t>
          Procedure: Execute the identical collective benchmark suite from <xref
          target="collective-bench"/> over UET RUD transport using a
          UEC-compliant collective library. Run under UET RUD + packet spray as
          the primary configuration, and UET ROD + ECMP as a secondary
          baseline.
        </t>
        <t>
          Reporting: Present a direct comparison table: UET RUD BusBW vs.
          RoCEv2 RC BusBW for each (collective, message_size, N) combination.
          Report the percentage improvement in BusBW and JCT attributable to
          UET's native spray tolerance and congestion control.
        </t>
      </section>

      <section anchor="uet-pdc-scale" numbered="true" toc="default">
        <name>UET PDC Scalability and Connection Setup Rate</name>
        <t>
          Objective: Measure the rate at which UET Packet Delivery Contexts
          (PDCs) can be established and the maximum concurrent PDC count,
          benchmarking the connectionless model's scalability advantage over
          RoCEv2 QP-based connections.
        </t>
        <t>
          Procedure: (a) PDC establishment rate: initiate PDC creation from a
          single NIC to M remote endpoints (M = {100, 1000, 10000, 100000}).
          (b) Data-before-handshake: measure first-byte latency for UET vs.
          RoCEv2 RDMA Write. (c) Maximum concurrent PDC count: progressively
          increase active PDCs until per-PDC throughput drops below 90% of the
          single-PDC rate. The UEC specification targets up to 1 million
          endpoints; verify the practical limit on the DUT NIC.
        </t>
        <t>
          Reporting: Report PDC establishment rate vs. QP establishment rate.
          Plot first-byte latency comparison. Report the maximum concurrent PDC
          count with its associated per-PDC throughput.
        </t>
      </section>
    </section>

    <!-- Section 7: Congestion Management -->
    <section anchor="congestion-bench" numbered="true" toc="default">
      <name>Test Category 2: Congestion Management</name>
      <t>
        AI training workloads generate repetitive micro-congestion during the
        back-propagation gradient synchronization phase. These tests evaluate
        how effectively the DUT fabric manages this congestion using ECN, PFC,
        and DCQCN mechanisms.
      </t>

      <section anchor="ecn-accuracy" numbered="true" toc="default">
        <name>ECN Marking Accuracy and Threshold</name>
        <t>
          Objective: Verify that the DUT marks packets with ECN Congestion
          Experienced (CE) at the configured threshold and with the correct
          granularity.
        </t>
        <t>
          Procedure: Configure an ECN marking threshold T on the DUT egress
          queue. The test <bcp14>MUST</bcp14> verify that (a) no packets are
          ECN-marked below threshold T, (b) 100% of packets are ECN-marked
          above the maximum threshold, and (c) the marking probability ramps
          appropriately between thresholds (per WRED/RED configuration). Repeat
          for multiple ECN threshold configurations: low (e.g., 100KB), medium
          (e.g., 1MB), high (e.g., 5MB).
        </t>
      </section>

      <section anchor="pfc-incast" numbered="true" toc="default">
        <name>PFC Behavior Under Incast</name>
        <t>
          Objective: Characterize the DUT's PFC generation behavior under N:1
          incast conditions representative of AI training AllReduce operations.
        </t>
        <t>
          Procedure: Generate N:1 incast traffic at 100% line rate from N
          sources to 1 destination, where N = {2, 4, 8, 16, 32, 64}. Measure
          PFC PAUSE frame count per second at each hop, PFC PAUSE duration per
          port, PFC storm onset, and end-to-end throughput. The test
          <bcp14>SHOULD</bcp14> verify that PFC headroom is correctly sized (no
          drops despite PFC activation) and that PFC watchdog mechanisms prevent
          persistent pauses.
        </t>
      </section>

      <section anchor="dcqcn-conv" numbered="true" toc="default">
        <name>DCQCN Convergence Time</name>
        <t>
          Objective: Measure the time for the DCQCN congestion control loop to
          converge to a fair-share rate after congestion onset.
        </t>
        <t>
          Procedure: Establish M flows through a common bottleneck link. At time
          T0, inject an additional M flows to create 2:1 oversubscription.
          Measure convergence time until all 2M flows achieve rates within 10%
          of fair share. Repeat for M = {4, 16, 64, 256}.
        </t>
      </section>

      <section anchor="pfc-deadlock" numbered="true" toc="default">
        <name>PFC Storm and Deadlock Resilience</name>
        <t>
          Objective: Verify that the DUT does not enter a PFC deadlock or
          sustained PFC storm under adversarial traffic patterns.
        </t>
        <t>
          Procedure: Generate cyclic traffic patterns known to cause PFC
          deadlocks in vulnerable implementations. Run for 300 seconds. The DUT
          <bcp14>MUST</bcp14> demonstrate resilience: either preventing the
          deadlock via PFC watchdog or via architectural immunity (e.g.,
          VOQ-based scheduling).
        </t>
      </section>
    </section>

    <!-- Section 8: Load Balancing -->
    <section anchor="lb-bench" numbered="true" toc="default">
      <name>Test Category 3: Load Balancing Efficacy</name>
      <t>
        Load balancing across parallel fabric paths is critical for AI training
        fabrics because the traffic consists of a small number of
        high-bandwidth, long-lived elephant flows. Traditional 5-tuple ECMP
        performs poorly in this regime due to low flow entropy.
      </t>

      <section anchor="ecmp-entropy" numbered="true" toc="default">
        <name>ECMP Entropy and Polarization</name>
        <t>
          Objective: Quantify the degree of traffic polarization under standard
          ECMP hashing for AI training flow patterns.
        </t>
        <t>
          Procedure: Configure the DUT with standard 5-tuple ECMP. Generate
          traffic with Q = {1, 4, 8, 16, 32} QPs per source-destination pair.
          Measure per-link utilization, MMR, and JFI. Test with and without
          BTH-aware hashing. Repeat for fabric sizes of 8, 16, 32, and 64 leaf
          switches.
        </t>
      </section>

      <section anchor="dlb-flowlet" numbered="true" toc="default">
        <name>Dynamic Load Balancing (Flowlet)</name>
        <t>
          Objective: Evaluate the DUT's flowlet-based DLB performance for AI
          training traffic and compare to baseline ECMP.
        </t>
        <t>
          Procedure: Configure the DUT with vendor-specific DLB. Generate
          traffic with Q=4 QPs. Measure MMR, JFI, per-link utilization, and
          out-of-order packet rate. Vary the flowlet gap timer and report
          sensitivity.
        </t>
      </section>

      <section anchor="pkt-spray" numbered="true" toc="default">
        <name>Packet Spraying</name>
        <t>
          Objective: Evaluate the DUT's per-packet spraying performance and
          quantify the tradeoff between utilization and packet reordering.
        </t>
        <t>
          Procedure: Configure the DUT for per-packet load balancing. Measure
          MMR (expected near 1.0), JFI (expected near 1.0), out-of-order packet
          rate, and the impact on RDMA retransmissions. If the DUT provides an
          in-fabric reorder buffer, document this capability per <xref
          target="appendix-asic"/>.
        </t>
      </section>

      <section anchor="jfi-measure" numbered="true" toc="default">
        <name>Jain Fairness Index Measurement</name>
        <t>
          Objective: Provide a single-number summary of load balancing quality
          comparable across all strategies.
        </t>
        <t>
          Procedure: For each load balancing strategy, compute the Jain
          Fairness Index (JFI) as:
        </t>
        <artwork align="center" name="" type="ascii-art"><![CDATA[
JFI = (sum(LinkTx_i))^2 / (N * sum(LinkTx_i^2))
        ]]></artwork>
        <t>
          where LinkTx_i is the transmitted traffic on fabric link i over the
          measurement interval, and N is the total number of parallel links.
          JFI ranges from 1/N (worst case: all traffic on one link) to 1.0
          (perfect distribution).
        </t>
      </section>
    </section>

    <!-- Section 9: Collective Communication -->
    <section anchor="collective-bench" numbered="true" toc="default">
      <name>Test Category 4: Collective Communication Benchmarks</name>
      <t>
        These tests evaluate the fabric's performance under realistic collective
        communication patterns generated by AI training frameworks. Unlike the
        synthetic RDMA tests in <xref target="rdma-bench"/> and <xref
        target="uet-bench"/>, these tests exercise the full communication stack
        including the collective library (NCCL, RCCL, or equivalent) and its
        interaction with the network.
      </t>

      <section anchor="allreduce-bench" numbered="true" toc="default">
        <name>AllReduce Benchmark</name>
        <t>
          Objective: Measure fabric performance during AllReduce operations, the
          dominant collective for gradient synchronization in data-parallel
          training.
        </t>
        <t>
          Procedure: Execute AllReduce (sum) operations with message sizes of
          1MB, 8MB, 64MB, 256MB, 1GB, and 4GB. Vary N = {8, 16, 32, 64, 128,
          256, 512, 1024} accelerators. Execute at least 100 iterations per
          (message_size, N) pair and report average, P50, P95, P99 BusBW. Run
          under each load balancing strategy (ECMP, DLB, spray).
        </t>
      </section>

      <section anchor="alltoall-bench" numbered="true" toc="default">
        <name>AlltoAll Benchmark</name>
        <t>
          Objective: Measure fabric performance during AlltoAll operations, used
          in Mixture-of-Experts (MoE) models and expert parallelism.
        </t>
        <t>
          Procedure: Execute AlltoAll operations with the same message size and
          N permutations as <xref target="allreduce-bench"/>. AlltoAll creates
          the worst-case congestion scenario because every accelerator
          simultaneously sends data to every other accelerator. Additionally,
          report the JCT for AlltoAll iterations, as AlltoAll JCT is the most
          sensitive indicator of fabric congestion management quality.
        </t>
      </section>

      <section anchor="allgather-bench" numbered="true" toc="default">
        <name>AllGather Benchmark</name>
        <t>
          Objective: Measure fabric performance during AllGather operations,
          used for parameter distribution in tensor-parallel and
          pipeline-parallel training.
        </t>
        <t>
          Procedure: Execute AllGather operations via the collective
          communication library with the same parameter sweeps as <xref
          target="allreduce-bench"/>. Measure BusBW, JCT, and fabric health
          indicators.
        </t>
      </section>

      <section anchor="ccl-summary" numbered="true" toc="default">
        <name>Collective Communication Library Bus Bandwidth Summary</name>
        <t>
          Objective: Provide a consolidated view of fabric-level collective
          performance across all operations.
        </t>
        <t>
          Reporting: Present a summary table indexed by load balancing
          configuration (UET RUD + Packet Spray, UET ROD + Packet Spray, RoCEv2
          RC + Packet Spray, RoCEv2 RC + ECMP, UET RUD + DLB/Flowlet) with
          columns for MMR, JFI, OOO Rate, Retx Rate, and Effective Goodput (%).
        </t>
      </section>
    </section>

    <!-- Section 10: JCT -->
    <section anchor="jct-bench" numbered="true" toc="default">
      <name>Test Category 5: Job Completion Time (JCT) Benchmarks</name>
      <t>
        JCT is the single most important user-facing KPI for AI training
        fabrics. It directly determines accelerator utilization and training
        cost.
      </t>

      <section anchor="synthetic-jct" numbered="true" toc="default">
        <name>Synthetic JCT Under Controlled Conditions</name>
        <t>
          Objective: Measure the JCT for a defined synthetic workload that
          exercises the fabric with a known computation-to-communication ratio,
          enabling isolation of fabric-induced overhead.
        </t>
        <t>
          Procedure: Define a synthetic training iteration as: (1) a computation
          phase of C milliseconds, followed by (2) a communication phase
          consisting of an AllReduce of S bytes across N accelerators. Vary C in
          {10ms, 50ms, 100ms, 500ms}, S in {256MB, 1GB, 4GB}, and N in {64,
          128, 256, 512, 1024}. Execute 1000 iterations and compute:
        </t>
        <artwork align="center" name="" type="ascii-art"><![CDATA[
Roofline JCT = Iterations * (C + S * algo_factor / NIC_line_rate)
JCT Ratio    = Measured_JCT / Roofline_JCT
        ]]></artwork>
        <t>
          NOTE: A JCT Ratio below 1.05 represents excellent fabric performance
          in current deployments; above 1.15 represents significant
          fabric-induced overhead. These are non-normative illustrative
          reference values only.
        </t>
      </section>

      <section anchor="mlperf-jct" numbered="true" toc="default">
        <name>MLPerf-Aligned JCT</name>
        <t>
          Objective: Measure JCT using MLPerf Training benchmark workloads to
          enable comparison with published industry results.
        </t>
        <t>
          Procedure: Execute MLPerf Training benchmark closed-division workloads
          (e.g., BERT, ResNet, GPT-3 175B) according to MLPerf submission rules.
          Simultaneously capture all fabric KPIs from <xref
          target="kpi-framework"/>. Report time-to-train and/or
          tokens-per-second as primary results.
        </t>
      </section>

      <section anchor="multitenant-jct" numbered="true" toc="default">
        <name>Multi-Tenant JCT Interference</name>
        <t>
          Objective: Quantify the impact on JCT when multiple training jobs
          share the same fabric (multi-tenancy).
        </t>
        <t>
          Procedure: Configure two or more independent training jobs on the same
          DUT fabric. Jobs <bcp14>SHOULD</bcp14> overlap in their use of
          spine-layer links. Measure JCT for each job independently (baseline),
          then simultaneously (contention). The JCT interference factor =
          Contention_JCT / Baseline_JCT. Test with varying degrees of overlap
          (0%, 25%, 50%, 75% of spine links shared).
        </t>
      </section>
    </section>

    <!-- Section 11: Scale and Convergence -->
    <section anchor="scale-bench" numbered="true" toc="default">
      <name>Test Category 6: Scale and Convergence</name>

      <section anchor="fabric-scale" numbered="true" toc="default">
        <name>Fabric Scale Limits</name>
        <t>
          Objective: Determine the maximum fabric scale at which the DUT
          maintains acceptable KPI performance.
        </t>
        <t>
          Procedure: Progressively increase active accelerator endpoints from
          N=64 to the maximum supported while running the AllReduce benchmark
          from <xref target="allreduce-bench"/> with S=1GB. Additionally,
          measure BGP/routing convergence time after clearing all adjacencies,
          analogous to the EVPN scale test methodology defined in <xref
          target="EVPN-BENCH"/> Section 6.
        </t>
      </section>

      <section anchor="link-failure" numbered="true" toc="default">
        <name>Link Failure Convergence</name>
        <t>
          Objective: Measure the traffic disruption and JCT impact when a fabric
          link fails during active training.
        </t>
        <t>
          Procedure: With the fabric fully loaded (AllReduce, N=128, S=1GB),
          administratively fail a spine uplink. Measure: (a) duration of packet
          loss, (b) packets lost, (c) JCT overhead for the failure iteration,
          (d) time for load balancing to redistribute flows. Repeat for leaf
          uplink failure, spine switch failure, and superspine link failure.
        </t>
      </section>

      <section anchor="zero-impact-fo" numbered="true" toc="default">
        <name>Zero-Impact Failover Measurement</name>
        <t>
          Objective: Verify vendor claims of zero-impact or sub-microsecond
          failover for AI training traffic.
        </t>
        <t>
          Procedure: Execute the test from <xref target="link-failure"/> with
          nanosecond-precision measurement. A failure is considered
          "zero-impact" if the measured JCT for the failure iteration is within
          the P99 JCT of steady-state iterations.
        </t>
      </section>
    </section>

    <!-- Section 12: Soak -->
    <section anchor="soak-bench" numbered="true" toc="default">
      <name>Test Category 7: Soak and Stability</name>

      <section anchor="soak-24h" numbered="true" toc="default">
        <name>24-Hour Sustained Load</name>
        <t>
          Objective: Verify DUT fabric stability under sustained AI training
          load over an extended period, following the methodology pattern from
          <xref target="EVPN-BENCH"/> Section 7.
        </t>
        <t>
          Procedure: Configure the DUT at maximum validated scale from <xref
          target="fabric-scale"/>. Generate bidirectional collective
          communication traffic (alternating AllReduce and AlltoAll). Run
          continuously for 24 hours. Sample all KPIs from <xref
          target="kpi-framework"/> every 60 seconds. There <bcp14>SHOULD
          NOT</bcp14> be any memory leaks, crashes, or CPU spikes. Any anomaly
          <bcp14>MUST</bcp14> be reported with timestamp and duration.
        </t>
      </section>

      <section anchor="resource-leak" numbered="true" toc="default">
        <name>Resource Leak Detection</name>
        <t>
          Objective: Detect memory leaks, handle exhaustion, or gradual
          performance degradation in DUT software.
        </t>
        <t>
          Procedure: Record per-process memory usage at T=0, T=1h, T=6h, T=12h,
          and T=24h. Compute the linear regression slope of memory usage over
          time. A slope exceeding 1MB/hour for any process indicates a potential
          memory leak and <bcp14>MUST</bcp14> be reported.
        </t>
      </section>
    </section>

    <!-- Section 13: Reporting Format -->
    <section anchor="reporting" numbered="true" toc="default">
      <name>Reporting Format</name>
      <t>Test reports <bcp14>MUST</bcp14> include the following sections:</t>
      <ol spacing="normal" type="1">
        <li>DUT Identification: Complete parameters from <xref target="dut-id"/> for all fabric components.</li>
        <li>Test Topology: Diagram and description per <xref target="ref-topologies"/>, including physical cabling.</li>
        <li>Test Configuration: All DUT configuration parameters including QoS policies (ECN thresholds, PFC headroom, DCQCN parameters), load balancing mode, buffer allocation, and any vendor-specific tuning applied.</li>
        <li>Host Configuration: Complete host stack description per <xref target="dut-id"/> including NIC firmware, driver, collective library version, and any tuning applied. For UET tests, additionally report: UEC compliance profile (AI Base, AI Full, or HPC), libfabric provider version, NIC UEC firmware version, and which optional link-layer features are enabled (LLR, Packet Trimming, PRI, CBFC).</li>
        <li>Test Results: For each test from <xref target="rdma-bench"/> through <xref target="soak-bench"/>, provide the specified reporting format including tables, graphs, and statistical summaries. For <xref target="uet-bench"/> tests, results <bcp14>MUST</bcp14> include side-by-side UET vs. RoCEv2 comparison data on the identical DUT fabric.</li>
        <li>Anomalies: Any deviations from the specified procedures, test failures, or unexpected behaviors <bcp14>MUST</bcp14> be documented.</li>
        <li>Repeatability Statement: For each test, report the number of iterations performed and the coefficient of variation (standard deviation / mean). A CV below 5% is <bcp14>RECOMMENDED</bcp14> for test validity.</li>
      </ol>
    </section>

    <!-- Section 14: Security -->
    <section anchor="security" numbered="true" toc="default">
      <name>Security Considerations</name>
      <t>
        This document defines benchmarking methodologies for controlled
        laboratory environments and does not introduce new security mechanisms
        or protocols. Per <xref target="RFC6815"/>, the tests defined herein
        <bcp14>MUST NOT</bcp14> be performed on production networks. The use of
        dedicated test IP address ranges per <xref target="RFC2544"/> Appendix C
        (198.18.0.0/15) is <bcp14>RECOMMENDED</bcp14> to prevent accidental
        interaction with production infrastructure.
      </t>
      <t>
        When RDMA/RoCEv2 traffic is used, the test environment
        <bcp14>SHOULD</bcp14> be isolated from production RDMA fabrics to
        prevent QP number space collisions or inadvertent PFC propagation. When
        UET traffic is used (<xref target="uet-bench"/>), the test environment
        <bcp14>MUST</bcp14> ensure that UDP port 4793 traffic does not leak to
        production networks, and that PDC identifier spaces are isolated. UET's
        optional transport security sub-layer (TSS) <bcp14>SHOULD NOT</bcp14>
        be enabled during performance benchmarking unless transport security
        overhead is explicitly being measured.
      </t>
    </section>

    <!-- Section 15: IANA -->
    <section anchor="iana" numbered="true" toc="default">
      <name>IANA Considerations</name>
      <t>This document makes no request of IANA.</t>
    </section>

  </middle>

  <!-- ====================================================== -->
  <!--  BACK MATTER                                            -->
  <!-- ====================================================== -->
  <back>

    <!-- References -->
    <references>
      <name>References</name>

      <references anchor="normative-refs">
        <name>Normative References</name>

        <reference anchor="RFC1242" target="https://www.rfc-editor.org/rfc/rfc1242">
          <front>
            <title>Benchmarking Terminology for Network Interconnection Devices</title>
            <author initials="S." surname="Bradner" fullname="S. Bradner"/>
            <date month="July" year="1991"/>
          </front>
          <seriesInfo name="RFC" value="1242"/>
          <seriesInfo name="DOI" value="10.17487/RFC1242"/>
        </reference>

        <reference anchor="RFC2119" target="https://www.rfc-editor.org/rfc/rfc2119">
          <front>
            <title>Key words for use in RFCs to Indicate Requirement Levels</title>
            <author initials="S." surname="Bradner" fullname="S. Bradner"/>
            <date month="March" year="1997"/>
          </front>
          <seriesInfo name="BCP" value="14"/>
          <seriesInfo name="RFC" value="2119"/>
          <seriesInfo name="DOI" value="10.17487/RFC2119"/>
        </reference>

        <reference anchor="RFC2544" target="https://www.rfc-editor.org/rfc/rfc2544">
          <front>
            <title>Benchmarking Methodology for Network Interconnect Devices</title>
            <author initials="S." surname="Bradner" fullname="S. Bradner"/>
            <author initials="J." surname="McQuaid" fullname="J. McQuaid"/>
            <date month="March" year="1999"/>
          </front>
          <seriesInfo name="RFC" value="2544"/>
          <seriesInfo name="DOI" value="10.17487/RFC2544"/>
        </reference>

        <reference anchor="RFC2889" target="https://www.rfc-editor.org/rfc/rfc2889">
          <front>
            <title>Benchmarking Methodology for LAN Switching Devices</title>
            <author initials="R." surname="Mandeville" fullname="R. Mandeville"/>
            <author initials="J." surname="Perser" fullname="J. Perser"/>
            <date month="August" year="2000"/>
          </front>
          <seriesInfo name="RFC" value="2889"/>
        </reference>

        <reference anchor="RFC8174" target="https://www.rfc-editor.org/rfc/rfc8174">
          <front>
            <title>Ambiguity of Uppercase vs Lowercase in RFC 2119 Key Words</title>
            <author initials="B." surname="Leiba" fullname="B. Leiba"/>
            <date month="May" year="2017"/>
          </front>
          <seriesInfo name="BCP" value="14"/>
          <seriesInfo name="RFC" value="8174"/>
          <seriesInfo name="DOI" value="10.17487/RFC8174"/>
        </reference>

        <reference anchor="RFC8238" target="https://www.rfc-editor.org/rfc/rfc8238">
          <front>
            <title>Data Center Benchmarking Terminology</title>
            <author initials="L." surname="Avramov" fullname="L. Avramov"/>
            <author initials="J." surname="Rapp" fullname="J. Rapp"/>
            <date month="August" year="2017"/>
          </front>
          <seriesInfo name="RFC" value="8238"/>
          <seriesInfo name="DOI" value="10.17487/RFC8238"/>
        </reference>

        <reference anchor="RFC8239" target="https://www.rfc-editor.org/rfc/rfc8239">
          <front>
            <title>Data Center Benchmarking Methodology</title>
            <author initials="L." surname="Avramov" fullname="L. Avramov"/>
            <author initials="J." surname="Rapp" fullname="J. Rapp"/>
            <date month="August" year="2017"/>
          </front>
          <seriesInfo name="RFC" value="8239"/>
          <seriesInfo name="DOI" value="10.17487/RFC8239"/>
        </reference>

        <reference anchor="RFC9004" target="https://www.rfc-editor.org/rfc/rfc9004">
          <front>
            <title>Updates for the Back-to-Back Frame Benchmark in RFC 2544</title>
            <author initials="A." surname="Morton" fullname="A. Morton"/>
            <date month="May" year="2021"/>
          </front>
          <seriesInfo name="RFC" value="9004"/>
          <seriesInfo name="DOI" value="10.17487/RFC9004"/>
        </reference>

        <reference anchor="UEC-1.0" target="https://ultraethernet.org">
          <front>
            <title>Ultra Ethernet Transport (UET) Specification 1.0</title>
            <author>
              <organization>Ultra Ethernet Consortium</organization>
            </author>
            <date month="June" year="2025"/>
          </front>
        </reference>
      </references>

      <references anchor="informative-refs">
        <name>Informative References</name>

        <reference anchor="RFC6815" target="https://www.rfc-editor.org/rfc/rfc6815">
          <front>
            <title>Applicability Statement for RFC 2544: Use on Production Networks Considered Harmful</title>
            <author initials="S." surname="Bradner" fullname="S. Bradner"/>
            <date month="November" year="2012"/>
          </front>
          <seriesInfo name="RFC" value="6815"/>
          <seriesInfo name="DOI" value="10.17487/RFC6815"/>
        </reference>

        <reference anchor="EVPN-BENCH">
          <front>
            <title>Benchmarking Methodology for EVPN and PBB-EVPN</title>
            <author initials="S." surname="Jacob" fullname="S. Jacob"/>
            <author initials="K." surname="Tiruveedhula" fullname="K. Tiruveedhula"/>
            <date month="June" year="2018"/>
          </front>
          <seriesInfo name="Internet-Draft" value="draft-kishjac-bmwg-evpntest-10"/>
        </reference>

        <reference anchor="LLM-BENCH">
          <front>
            <title>Benchmarking Methodology for Large Language Model Serving</title>
            <author initials="G." surname="Gaikwad" fullname="G. Gaikwad"/>
            <date month="January" year="2026"/>
          </front>
          <seriesInfo name="Internet-Draft" value="draft-gaikwad-llm-benchmarking-methodology-00"/>
        </reference>

        <reference anchor="META-ROCE">
          <front>
            <title>RDMA over Ethernet for Distributed AI Training at Meta Scale</title>
            <author initials="A." surname="Gangidi" fullname="A. Gangidi"/>
            <date year="2024"/>
          </front>
          <seriesInfo name="ACM" value="SIGCOMM 2024"/>
        </reference>

        <reference anchor="DCQCN">
          <front>
            <title>Congestion Control for Large-Scale RDMA Deployments</title>
            <author initials="Y." surname="Zhu" fullname="Y. Zhu"/>
            <date year="2015"/>
          </front>
          <seriesInfo name="ACM" value="SIGCOMM 2015"/>
        </reference>

        <reference anchor="NCCL" target="https://developer.nvidia.com/nccl">
          <front>
            <title>NVIDIA Collective Communications Library (NCCL)</title>
            <author>
              <organization>NVIDIA</organization>
            </author>
            <date year="2024"/>
          </front>
        </reference>

        <reference anchor="LIBFABRIC" target="https://ofiwg.github.io/libfabric/">
          <front>
            <title>libfabric: Open Fabric Interfaces</title>
            <author>
              <organization>OpenFabrics Interfaces Working Group</organization>
            </author>
            <date year="2024"/>
          </front>
        </reference>

        <reference anchor="MLPERF">
          <front>
            <title>MLPerf Training Benchmark Suite</title>
            <author>
              <organization>MLCommons</organization>
            </author>
            <date year="2024"/>
          </front>
        </reference>
      </references>
    </references>

    <!-- Appendix A -->
    <section anchor="appendix-kpi" numbered="true" toc="default">
      <name>KPI-to-Test Mapping Summary</name>
      <table anchor="tbl-kpi-map" align="left">
        <name>KPI-to-Test Mapping</name>
        <thead>
          <tr><th>KPI</th><th>Test Section</th><th>Measurement Method</th><th>Reporting Unit</th></tr>
        </thead>
        <tbody>
          <tr><td>Throughput Rate</td><td><xref target="rdma-throughput"/></td><td>Binary search, zero-loss</td><td>Tbps, % line rate</td></tr>
          <tr><td>Latency (P99)</td><td><xref target="rdma-latency"/></td><td>Tagged frame, loaded / unloaded</td><td>microseconds</td></tr>
          <tr><td>Burst Absorption</td><td><xref target="rdma-burst"/></td><td>Max burst without loss</td><td>frames, bytes</td></tr>
          <tr><td>ECN Accuracy</td><td><xref target="ecn-accuracy"/></td><td>Queue depth vs. marking</td><td>threshold deviation %</td></tr>
          <tr><td>PFC Behavior</td><td><xref target="pfc-incast"/></td><td>Incast sweep N=2..64</td><td>PAUSE events/sec, duration</td></tr>
          <tr><td>DCQCN Convergence</td><td><xref target="dcqcn-conv"/></td><td>Rate stabilization after onset</td><td>microseconds</td></tr>
          <tr><td>PFC Deadlock</td><td><xref target="pfc-deadlock"/></td><td>Cyclic adversarial traffic</td><td>observed/reported, watchdog events</td></tr>
          <tr><td>ECMP Imbalance</td><td><xref target="ecmp-entropy"/></td><td>MMR, JFI per QP count</td><td>dimensionless ratios</td></tr>
          <tr><td>DLB Efficacy</td><td><xref target="dlb-flowlet"/></td><td>Throughput delta vs. ECMP</td><td>%, out-of-order rate</td></tr>
          <tr><td>Spray Efficacy</td><td><xref target="pkt-spray"/></td><td>JFI, retransmission rate</td><td>dimensionless, retx/sec</td></tr>
          <tr><td>AllReduce BusBW</td><td><xref target="allreduce-bench"/></td><td>CCL benchmark</td><td>Gbps per accelerator</td></tr>
          <tr><td>AlltoAll JCT</td><td><xref target="alltoall-bench"/></td><td>CCL benchmark</td><td>seconds per iteration</td></tr>
          <tr><td>AllGather BusBW</td><td><xref target="allgather-bench"/></td><td>CCL benchmark</td><td>Gbps per accelerator</td></tr>
          <tr><td>Synthetic JCT Ratio</td><td><xref target="synthetic-jct"/></td><td>Measured / Roofline</td><td>dimensionless</td></tr>
          <tr><td>MLPerf JCT</td><td><xref target="mlperf-jct"/></td><td>Time-to-train</td><td>minutes, tokens/sec</td></tr>
          <tr><td>Multi-Tenant Impact</td><td><xref target="multitenant-jct"/></td><td>Contention / Baseline JCT</td><td>interference factor</td></tr>
          <tr><td>Scale Limit</td><td><xref target="fabric-scale"/></td><td>Max N with JCT Ratio characterization</td><td>accelerator count</td></tr>
          <tr><td>Failover Time</td><td><xref target="link-failure"/></td><td>Loss duration on link fail</td><td>microseconds</td></tr>
          <tr><td>24h Stability</td><td><xref target="soak-24h"/></td><td>JCT Ratio std deviation</td><td>dimensionless</td></tr>
          <tr><td>UET Throughput (RUD)</td><td><xref target="uet-throughput"/></td><td>Binary search per transport service</td><td>Gbps, % line rate</td></tr>
          <tr><td>UET First-Packet Latency</td><td><xref target="uet-latency"/></td><td>PDC establish + first data</td><td>microseconds</td></tr>
          <tr><td>UET Spray Efficacy</td><td><xref target="uet-spray"/></td><td>JFI/MMR under RUD spray</td><td>dimensionless, OOO rate</td></tr>
          <tr><td>UET PFC-Free Loss Rate</td><td><xref target="uet-cc"/></td><td>Incast without PFC enabled</td><td>%, retx overhead</td></tr>
          <tr><td>LLR Retry Latency</td><td><xref target="uet-lle"/></td><td>Per-hop error recovery time</td><td>nanoseconds</td></tr>
          <tr><td>Packet Trimming Savings</td><td><xref target="uet-lle"/></td><td>BW saved during congestion</td><td>% bandwidth</td></tr>
          <tr><td>CBFC vs PFC HOL Blocking</td><td><xref target="uet-lle"/></td><td>Head-of-line blocking duration</td><td>microseconds</td></tr>
          <tr><td>UET Collective BusBW</td><td><xref target="uet-collective"/></td><td>AllReduce/AlltoAll over UET</td><td>Gbps per accelerator</td></tr>
          <tr><td>PDC Establishment Rate</td><td><xref target="uet-pdc-scale"/></td><td>Sustained PDC creation rate</td><td>PDCs/second</td></tr>
          <tr><td>Max Concurrent PDCs</td><td><xref target="uet-pdc-scale"/></td><td>Scale limit per NIC</td><td>count</td></tr>
        </tbody>
      </table>
    </section>

    <!-- Appendix B -->
    <section anchor="appendix-asic" numbered="true" toc="default">
      <name>ASIC Feature Categories for AI Fabric Benchmarking (Informational)</name>
      <t>
        This appendix identifies the ASIC feature categories relevant to AI
        fabric performance. Implementers <bcp14>SHOULD</bcp14> document which of
        the following categories are present and enabled on the DUT during
        benchmarking. Specific vendor names and products are intentionally
        omitted.
      </t>
      <table anchor="tbl-asic" align="left">
        <name>ASIC Feature Categories</name>
        <thead>
          <tr><th>Feature Category</th><th>Sub-types</th><th>Relevance to AI Fabric</th><th>What to Report</th></tr>
        </thead>
        <tbody>
          <tr>
            <td>Aggregate Switching BW</td>
            <td>ASIC-level capacity</td>
            <td>Determines cluster scale, bisection BW</td>
            <td>Total Tbps; per-port speed (400/800GbE)</td>
          </tr>
          <tr>
            <td>Buffer Architecture</td>
            <td>Shared, VOQ, Cut-through</td>
            <td>Microburst absorption, PFC behavior, lossless operation</td>
            <td>Buffer type; total bytes; shared vs. dedicated split; per-port/queue allocation</td>
          </tr>
          <tr>
            <td>Packet Distribution</td>
            <td>Per-flow, Per-packet, Flowlet</td>
            <td>ECMP load balancing quality and reordering risk</td>
            <td>Supported granularities; in-fabric reorder buffer present (yes/no)</td>
          </tr>
          <tr>
            <td>Congestion Control</td>
            <td>ECN marking, PFC, DCQCN</td>
            <td>DCQCN convergence and lossless behavior</td>
            <td>ECN granularity (port/queue/VOQ); PFC priorities; DCQCN parameter range</td>
          </tr>
          <tr>
            <td>Adaptive Routing</td>
            <td>Flowlet, ECMP, Spray, Topology-aware</td>
            <td>Load balancing quality under collective patterns</td>
            <td>Algorithm type; flowlet gap timer range; topology-aware support</td>
          </tr>
          <tr>
            <td>Telemetry</td>
            <td>Per-port, Per-queue, Per-flow</td>
            <td>Required for KPI measurement during benchmarking</td>
            <td>Monitoring granularity; streaming interval; INT support</td>
          </tr>
          <tr>
            <td>Cluster Scale Support</td>
            <td>2-tier, 3-tier</td>
            <td>Applicable topology scales</td>
            <td>Max cluster size per topology; ASIC count</td>
          </tr>
        </tbody>
      </table>
      <t>
        All values <bcp14>MUST</bcp14> be reported based on vendor
        documentation or measured capability during benchmarking. The feature
        categories above are not exhaustive; additional DUT capabilities that
        affect benchmark results <bcp14>MUST</bcp14> also be documented.
      </t>
    </section>

    <!-- Appendix C -->
    <section anchor="appendix-rocev2-frame" numbered="true" toc="default">
      <name>RoCEv2 Test Frame Format</name>
      <table anchor="tbl-rocev2-frame" align="left">
        <name>RoCEv2 Test Frame Format</name>
        <thead>
          <tr><th>Offset</th><th>Field</th><th>Size</th><th>Value / Description</th></tr>
        </thead>
        <tbody>
          <tr><td>00</td><td>Ethernet Dst MAC</td><td>6B</td><td>DUT next-hop MAC</td></tr>
          <tr><td>06</td><td>Ethernet Src MAC</td><td>6B</td><td>Test equipment MAC</td></tr>
          <tr><td>12</td><td>EtherType</td><td>2B</td><td>0x8100 (802.1Q) or 0x0800 (IPv4)</td></tr>
          <tr><td>14</td><td>VLAN Tag (optional)</td><td>4B</td><td>PCP=3 (RoCEv2 priority), VID</td></tr>
          <tr><td>18</td><td>IPv4 Header</td><td>20B</td><td>DSCP=26 (ECN-capable), Proto=17 (UDP)</td></tr>
          <tr><td>38</td><td>UDP Header</td><td>8B</td><td>DstPort=4791 (RoCEv2), SrcPort=var</td></tr>
          <tr><td>46</td><td>BTH (Base Transport Header)</td><td>12B</td><td>OpCode, DstQP, PSN, P_Key</td></tr>
          <tr><td>58</td><td>RETH (if Write)</td><td>16B</td><td>VA, R_Key, DMA Length</td></tr>
          <tr><td>74</td><td>Payload</td><td>var</td><td>Test data (incrementing octets)</td></tr>
          <tr><td>var</td><td>ICRC</td><td>4B</td><td>Invariant CRC</td></tr>
          <tr><td>var+4</td><td>FCS</td><td>4B</td><td>Ethernet Frame Check Sequence</td></tr>
        </tbody>
      </table>
    </section>

    <!-- Appendix D -->
    <section anchor="appendix-uet-frame" numbered="true" toc="default">
      <name>UET (Ultra Ethernet Transport) Frame Format</name>
      <t>
        The test frame for UET benchmarking follows the UEC Specification 1.0
        wire format. UET runs over UDP/IP using IANA-assigned destination port
        4793.
      </t>
      <table anchor="tbl-uet-frame" align="left">
        <name>UET Test Frame Format</name>
        <thead>
          <tr><th>Offset</th><th>Field</th><th>Size</th><th>Value / Description</th></tr>
        </thead>
        <tbody>
          <tr><td>00</td><td>Ethernet Dst MAC</td><td>6B</td><td>DUT next-hop MAC</td></tr>
          <tr><td>06</td><td>Ethernet Src MAC</td><td>6B</td><td>Test equipment MAC</td></tr>
          <tr><td>12</td><td>EtherType</td><td>2B</td><td>0x8100 (802.1Q) or 0x0800 (IPv4)</td></tr>
          <tr><td>14</td><td>VLAN Tag (optional)</td><td>4B</td><td>PCP=3 (UET priority class), VID</td></tr>
          <tr><td>18</td><td>IPv4 Header</td><td>20B</td><td>DSCP=26, ECN=ECT(0), Proto=17 (UDP)</td></tr>
          <tr><td>38</td><td>UDP Header</td><td>8B</td><td>DstPort=4793 (UET), SrcPort=entropy</td></tr>
          <tr><td>46</td><td>UET Common Header</td><td>16B</td><td>Version, OpCode, PDC ID, PSN, Entropy Value, Flags</td></tr>
          <tr><td>62</td><td>SES Header (Semantic)</td><td>var</td><td>Operation-specific (Write/Send/etc.)</td></tr>
          <tr><td>var</td><td>PDS Header (Pkt Delivery)</td><td>var</td><td>Sequence, Credit, Ack fields</td></tr>
          <tr><td>var</td><td>CMS Header (Cong. Mgmt)</td><td>var</td><td>ECN feedback, rate signals</td></tr>
          <tr><td>var</td><td>Payload</td><td>var</td><td>Application data</td></tr>
          <tr><td>var</td><td>ICRC</td><td>4B</td><td>Invariant CRC</td></tr>
          <tr><td>var+4</td><td>FCS</td><td>4B</td><td>Ethernet Frame Check Sequence</td></tr>
        </tbody>
      </table>

      <t>Key differences from RoCEv2 (<xref target="appendix-rocev2-frame"/>):</t>
      <ol spacing="normal" type="1">
        <li>UDP Destination Port: UET uses port 4793 vs. RoCEv2 port 4791.</li>
        <li>Entropy Value: The UET common header includes an explicit entropy field for ECMP path selection. The test equipment <bcp14>MUST</bcp14> vary this field to achieve uniform path distribution.</li>
        <li>Transport Service Indicator: The UET header encodes the transport service (ROD, RUD, RUDI, UUD) in the OpCode/Flags fields. Tests <bcp14>MUST</bcp14> set this field to match the service being benchmarked.</li>
        <li>PDC Identifier: The connectionless PDC ID field replaces RoCEv2's Destination QP. PDC IDs are ephemeral and may be reused; the test equipment <bcp14>MUST</bcp14> track PDC lifecycle for accurate measurement.</li>
        <li>Layered Sub-Headers: UET uses four sub-layers (SES, PDS, CMS, TSS) with variable-length headers. Implementations <bcp14>MUST</bcp14> follow <xref target="UEC-1.0"/> Section 4 for wire format details.</li>
        <li>Optional Link Layer Headers: When LLR, Packet Trimming, or PRI features are enabled, additional link-layer framing may be present. The test equipment <bcp14>MUST</bcp14> be configured to recognize and correctly parse these optional headers.</li>
      </ol>

      <table anchor="tbl-rocev2-uet-compare" align="left">
        <name>RoCEv2 vs. UET Header Field Comparison</name>
        <thead>
          <tr><th>Field</th><th>RoCEv2 Value</th><th>UET Value</th><th>Notes</th></tr>
        </thead>
        <tbody>
          <tr><td>UDP Dst Port</td><td>4791</td><td>4793</td><td>IANA-assigned for each protocol</td></tr>
          <tr><td>Transport Endpoint</td><td>QP Number (24b)</td><td>PDC ID (variable)</td><td>Connectionless in UET</td></tr>
          <tr><td>Sequence Number</td><td>PSN (24b)</td><td>PSN (extended)</td><td>Larger range in UET for RUD OOO tolerance</td></tr>
          <tr><td>Congestion Signal</td><td>ECN bits only</td><td>ECN + CMS sub-header</td><td>Sender + receiver signals in UET</td></tr>
          <tr><td>Entropy Source</td><td>UDP src port</td><td>Explicit entropy field</td><td>Deterministic spray in UET</td></tr>
          <tr><td>Ordering Guarantee</td><td>Always in-order (RC)</td><td>Per-service (ROD/RUD)</td><td>RUD allows OOO delivery</td></tr>
          <tr><td>Min Header Overhead</td><td>~74B (Write)</td><td>~78B (est. Write)</td><td>Slight increase for sub-layer headers</td></tr>
        </tbody>
      </table>
    </section>

  </back>
</rfc>
